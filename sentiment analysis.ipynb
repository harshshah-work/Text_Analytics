{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Phrase   Index\n",
      "0       A series of escapades demonstrating the adage ...       0\n",
      "1       A series of escapades demonstrating the adage ...       1\n",
      "2                                                A series       2\n",
      "3                                                       A       3\n",
      "4                                                  series       4\n",
      "5       of escapades demonstrating the adage that what...       5\n",
      "6                                                      of       6\n",
      "7       escapades demonstrating the adage that what is...       7\n",
      "8                                               escapades       8\n",
      "9       demonstrating the adage that what is good for ...       9\n",
      "10                                demonstrating the adage      10\n",
      "11                                          demonstrating      11\n",
      "12                                              the adage      12\n",
      "13                                                    the      13\n",
      "14                                                  adage      14\n",
      "15                        that what is good for the goose      15\n",
      "16                                                   that      16\n",
      "17                             what is good for the goose      17\n",
      "18                                                   what      18\n",
      "19                                  is good for the goose      19\n",
      "20                                                     is      20\n",
      "21                                     good for the goose      21\n",
      "22                                                   good      22\n",
      "23                                          for the goose      23\n",
      "24                                                    for      24\n",
      "25                                              the goose      25\n",
      "26                                                  goose      26\n",
      "27      is also good for the gander , some of which oc...      27\n",
      "28      is also good for the gander , some of which oc...      28\n",
      "29                                                is also      29\n",
      "...                                                   ...     ...\n",
      "156030                        a joke in the United States  156030\n",
      "156031  The movie 's downfall is to substitute plot fo...  156031\n",
      "156032                              The movie 's downfall  156032\n",
      "156033            is to substitute plot for personality .  156033\n",
      "156034              is to substitute plot for personality  156034\n",
      "156035                 to substitute plot for personality  156035\n",
      "156036                    substitute plot for personality  156036\n",
      "156037                                    substitute plot  156037\n",
      "156038                                    for personality  156038\n",
      "156039  The film is darkly atmospheric , with Herrmann...  156039\n",
      "156040  is darkly atmospheric , with Herrmann quietly ...  156040\n",
      "156041  is darkly atmospheric , with Herrmann quietly ...  156041\n",
      "156042                            is darkly atmospheric ,  156042\n",
      "156043                              is darkly atmospheric  156043\n",
      "156044  with Herrmann quietly suggesting the sadness a...  156044\n",
      "156045  Herrmann quietly suggesting the sadness and ob...  156045\n",
      "156046                                           Herrmann  156046\n",
      "156047  quietly suggesting the sadness and obsession b...  156047\n",
      "156048  suggesting the sadness and obsession beneath H...  156048\n",
      "156049               suggesting the sadness and obsession  156049\n",
      "156050                          the sadness and obsession  156050\n",
      "156051                              sadness and obsession  156051\n",
      "156052                                        sadness and  156052\n",
      "156053        beneath Hearst 's forced avuncular chortles  156053\n",
      "156054                Hearst 's forced avuncular chortles  156054\n",
      "156055                                          Hearst 's  156055\n",
      "156056                          forced avuncular chortles  156056\n",
      "156057                                 avuncular chortles  156057\n",
      "156058                                          avuncular  156058\n",
      "156059                                           chortles  156059\n",
      "\n",
      "[156060 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data= pd.read_csv('train.tsv',sep='\\t')\n",
    "\n",
    "data.head(10)\n",
    "phrase=data[['Phrase']]\n",
    "phrase['Index']=phrase.index\n",
    "new=phrase\n",
    "print(new)\n",
    "\n"
   ]
  },
  
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "words=word_tokenize(new['Phrase'].values)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Senticount=data.groupby('Sentiment').count()\n",
    "\n",
    "plt.bar(Senticount.index.values,Senticount['Phrase'])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12673)\t1\n",
      "  (0, 584)\t1\n",
      "  (0, 593)\t1\n",
      "  (0, 9065)\t1\n",
      "  (0, 5512)\t1\n",
      "  (0, 5751)\t1\n",
      "  (0, 5735)\t2\n",
      "  (0, 294)\t1\n",
      "  (0, 3444)\t1\n",
      "  (0, 4517)\t1\n",
      "  (0, 11671)\t1\n",
      "  (1, 5751)\t1\n",
      "  (1, 5735)\t1\n",
      "  (1, 294)\t1\n",
      "  (1, 3444)\t1\n",
      "  (1, 4517)\t1\n",
      "  (1, 11671)\t1\n",
      "  (2, 11671)\t1\n",
      "  (4, 11671)\t1\n",
      "  (5, 5751)\t1\n",
      "  (5, 5735)\t1\n",
      "  (5, 294)\t1\n",
      "  (5, 3444)\t1\n",
      "  (5, 4517)\t1\n",
      "  (7, 5751)\t1\n",
      "  :\t:\n",
      "  (156050, 9054)\t1\n",
      "  (156050, 11305)\t1\n",
      "  (156051, 9054)\t1\n",
      "  (156051, 11305)\t1\n",
      "  (156052, 11305)\t1\n",
      "  (156053, 2271)\t1\n",
      "  (156053, 1006)\t1\n",
      "  (156053, 6156)\t1\n",
      "  (156053, 5252)\t1\n",
      "  (156053, 1281)\t1\n",
      "  (156053, 11281)\t1\n",
      "  (156054, 2271)\t1\n",
      "  (156054, 1006)\t1\n",
      "  (156054, 6156)\t1\n",
      "  (156054, 5252)\t1\n",
      "  (156054, 11281)\t1\n",
      "  (156055, 6156)\t1\n",
      "  (156055, 11281)\t1\n",
      "  (156056, 2271)\t1\n",
      "  (156056, 1006)\t1\n",
      "  (156056, 5252)\t1\n",
      "  (156057, 2271)\t1\n",
      "  (156057, 1006)\t1\n",
      "  (156058, 1006)\t1\n",
      "  (156059, 2271)\t1\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase='True', max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "token=RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv= CountVectorizer(lowercase='True', stop_words='english',ngram_range=(1,1),tokenizer=token.tokenize)\n",
    "\n",
    "textcount=cv.fit_transform(data['Phrase'])\n",
    "print(textcount)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range=(1,1),tokenizer=token.tokenize)\n",
    "\n",
    "text_counts = cv.fit_transform(data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X_train,X_test,Y_train,Y_test= train_test_split(text_counts,data['Sentiment'],test_size=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6135460720235807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "clf=MultinomialNB().fit(X_train,Y_train)\n",
    "\n",
    "predicted=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(Y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf=TfidfVectorizer(lowercase=True,tokenizer=token.tokenize,ngram_range=(1,1))\n",
    "\n",
    "value=tf.fit_transform(data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6137383057798282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "final=MultinomialNB().fit(X_train,Y_train)\n",
    "\n",
    "fpredicted=final.predict(X_test)\n",
    "print(metrics.accuracy_score(Y_test,fpredicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'we', 'himself', \"shan't\", 'ours', 'their', 'too', 'isn', 'below', 'being', 'them', \"that'll\", 'needn', 'out', 'if', 'wasn', 'against', 'why', 'more', \"weren't\", 'yourselves', 'm', 'own', 'they', 'over', 'both', 'in', 'during', 'didn', \"won't\", 'those', 'the', \"you'll\", 'was', 'now', 'ourselves', 'whom', 'up', 'how', 'an', 'between', 'll', 'be', 'myself', 'at', 'here', 'nor', \"wouldn't\", 'yourself', 'am', 'on', 'any', 'before', 'few', 'her', 'hers', 'very', 'were', 'when', \"haven't\", \"needn't\", 'yours', \"couldn't\", 'again', \"aren't\", 'through', \"it's\", 'she', 'then', 'weren', 'should', 'shouldn', 'from', 'been', 'further', 'your', 'with', 'my', 'most', 'that', 'there', 'don', 'where', 'mightn', 'couldn', 'its', 'of', 'aren', 'd', 'because', 'y', 'this', 'such', 'you', 'has', 'which', 'will', 'him', 'is', 'itself', 'above', 'only', \"mightn't\", 'and', 'some', 'did', 'herself', \"you'd\", 'all', 'while', 'having', 'can', 'what', 'shan', 'theirs', 't', 'or', 'ain', 'once', 'won', 'hasn', 'down', 'themselves', 'o', 'do', 's', 've', 'to', 'doing', 'under', 'hadn', \"hadn't\", \"don't\", 'our', 'so', \"mustn't\", 'not', 'about', \"shouldn't\", 'after', 'does', 'mustn', 'are', 'just', 'haven', 'who', 'same', 'have', 'other', 'he', \"should've\", \"didn't\", \"wasn't\", 'by', 'i', 'off', \"she's\", 'than', 'no', 'each', 'wouldn', 'for', \"isn't\", 'until', 're', 'these', 'but', 'ma', 'doesn', \"you've\", 'me', \"you're\", 'into', \"hasn't\", 'had', \"doesn't\", 'a', 'his', 'as', 'it'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#stop_words=set(stop_words.words('english'))\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-65dc57cba23d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Phrase'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#newphrase= phrase.lloc()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize,word_tokenize\n",
    "\n",
    "phrase=data['Phrase'].values[0]\n",
    "\n",
    "words=[]\n",
    "\n",
    "for i in data['Phrase'].values.iteritems():\n",
    "    words.append(word_tokenize(i))\n",
    "#newphrase= phrase.lloc()\n",
    "#print(newphrase)\n",
    "\n",
    "\n",
    "print(words)\n",
    "#print(word_tokenize(phrase))\n",
    "\n",
    "\n",
    "#data_text = data[['Phrase']]\n",
    "#data_text['index'] = data_text.index\n",
    "#documents = data_text\n",
    "#print(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-7c2603474bd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in data['Phrase']:\n",
    "        if token not in  and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv=CountVectorizer(lowercase=True,stop_words='english',ngram_range=(1,1),tokenizer=token.tokenize)\n",
    "\n",
    "final=cv.fit_transform(data['Phrase'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(final,data['Sentiment'],test_size=0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6138023836985774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "predicted=MultinomialNB().fit(x_train,y_train)\n",
    "result=predicted.predict(x_test)\n",
    "\n",
    "print(metrics.accuracy_score(result,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.tokenize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-582b6bb33111>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.tokenize'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "token=RegexpTokenizer\n",
    "cv=CountVectorizer()\n",
    "\n",
    "words=cv.fit_transform(data['Phrase'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-0443bf79b5cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(words,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CountVectorizor' from 'sklearn.feature_extraction.text' (C:\\Users\\Harsh\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-463633670588>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnlkt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[a-zA-Z0-9]+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CountVectorizor' from 'sklearn.feature_extraction.text' (C:\\Users\\Harsh\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizor\n",
    "from nlkt.tokenizer import RegexpTokenizer\n",
    "\n",
    "token=RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "cv=CounterVectorizer(lowercase=True,stop_words='english',ngram_range=(1,1),tokenizer=token.tokenize)\n",
    "\n",
    "words=cv.fit_tranform(data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 156060]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-0208967c5a68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2182\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2184\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 235\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 156060]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(words,data['Sentiment'],test_size=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "final=MultinomialNB().fit(x_train,y_train)\n",
    "predction = final.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(Y_test,predction))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
